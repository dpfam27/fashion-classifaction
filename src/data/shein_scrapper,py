import requests
from bs4 import BeautifulSoup
import pandas as pd
import os
import time
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import random

class SheinScraper:
    def __init__(self):
        # Define paths
        self.base_dir = os.path.join('data', 'raw', 'shein_data')
        self.train_dir = os.path.join(self.base_dir, 'train')
        self.test_dir = os.path.join(self.base_dir, 'test')
        
        # Create directories if they don't exist
        for dir in [
            self.train_dir, 
            self.test_dir,
            os.path.join(self.train_dir, 'images'),
            os.path.join(self.test_dir, 'images')
        ]:
            os.makedirs(dir, exist_ok=True)
        
        # Enhanced configuration
        self.categories = {
            'dresses': '/category/Dresses-sc-00100089.html',
            'tops': '/category/Tops-sc-00101974.html',
            'bottoms': '/category/Bottoms-sc-00089121.html'
        }
        
        # Setup webdriver with options for better performance
        options = webdriver.ChromeOptions()
        options.add_argument('--disable-gpu')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-notifications')
        self.driver = webdriver.Chrome(options=options)
        self.base_url = "https://us.shein.com"  # Changed to US site for better stability
        
        # Add retry configuration
        self.max_retries = 3
        self.retry_delay = 5
        
    def scrape_category(self, category, target_count=1000):
        """Scrape products from a category with improved error handling and progress tracking"""
        url = f"{self.base_url}{self.categories[category]}"
        products = []
        page = 1
        
        try:
            while len(products) < target_count:
                paginated_url = f"{url}?page={page}"
                self.driver.get(paginated_url)
                
                # Wait for products with explicit timeout
                try:
                    WebDriverWait(self.driver, 15).until(
                        EC.presence_of_all_elements_located(
                            (By.CLASS_NAME, 'S-product-item')
                        )
                    )
                except Exception as e:
                    print(f"Timeout waiting for products on page {page}: {e}")
                    if page > 1:  # If not first page, might be end of products
                        break
                    continue
                
                # Find all products with retry mechanism
                for attempt in range(self.max_retries):
                    try:
                        items = self.driver.find_elements(
                            By.CLASS_NAME, 'S-product-item'
                        )
                        break
                    except Exception as e:
                        if attempt == self.max_retries - 1:
                            print(f"Failed to find products after {self.max_retries} attempts")
                            raise e
                        time.sleep(self.retry_delay)
                
                new_products = self._extract_products(items, category, target_count - len(products))
                products.extend(new_products)
                
                print(f"Category {category}: {len(products)}/{target_count} products scraped")
                
                if not new_products:  # No new products found
                    break
                    
                page += 1
                # Random delay between pages to avoid detection
                time.sleep(random.uniform(2, 4))
                
        except Exception as e:
            print(f"Error scraping category {category}: {e}")
            
        return products
    
    def _extract_products(self, items, category, remaining_count):
        """Extract product information with improved error handling"""
        products = []
        
        for item in items[:remaining_count]:
            try:
                # Wait for specific elements
                WebDriverWait(item, 5).until(
                    EC.presence_of_element_located((By.CLASS_NAME, 'S-product-item__name'))
                )
                
                product = {
                    'title': item.find_element(By.CLASS_NAME, 'S-product-item__name').text.strip(),
                    'price': item.find_element(By.CLASS_NAME, 'S-product-item__price').text.strip(),
                    'image_url': item.find_element(By.TAG_NAME, 'img').get_attribute('src'),
                    'product_url': item.find_element(By.TAG_NAME, 'a').get_attribute('href'),
                    'category': category,
                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                }
                
                # Validate product data
                if all(product.values()):
                    products.append(product)
                
            except Exception as e:
                print(f"Error extracting product: {e}")
                continue
                
        return products
    
    def _download_image(self, url, path):
        """Download image with retry mechanism"""
        for attempt in range(self.max_retries):
            try:
                response = requests.get(url, timeout=10)
                if response.status_code == 200:
                    with open(path, 'wb') as f:
                        f.write(response.content)
                    return True
                elif attempt == self.max_retries - 1:
                    print(f"Failed to download image after {self.max_retries} attempts: {url}")
                    return False
            except Exception as e:
                if attempt == self.max_retries - 1:
                    print(f"Error downloading image {url}: {e}")
                    return False
            time.sleep(self.retry_delay)
        return False
    
    def save_data(self, products):
        """Split data into train/test and save"""
        # Shuffle products
        random.shuffle(products)
        split_idx = len(products) // 2
        
        # Split into train/test
        train_products = products[:split_idx]
        test_products = products[split_idx:]
        
        # Save train data
        self._save_subset(train_products, 'train')
        # Save test data
        self._save_subset(test_products, 'test')
        
    def _save_subset(self, products, subset='train'):
        """Save a subset (train/test) of products"""
        dir_path = self.train_dir if subset == 'train' else self.test_dir
        
        # Save images
        for idx, product in enumerate(products):
            image_path = os.path.join(dir_path, 'images', f'{subset}_{idx}.jpg')
            self._download_image(product['image_url'], image_path)
            product['local_path'] = image_path
            
        # Save metadata
        df = pd.DataFrame(products)
        df.to_csv(os.path.join(dir_path, f'{subset}.csv'), index=False)
        
    def run(self):
        """Run complete scraping process with progress tracking"""
        all_products = []
        
        for category in self.categories:
            print(f"\nStarting to scrape category: {category}")
            products = self.scrape_category(category, target_count=1000)
            all_products.extend(products)
            
            print(f"Completed {category}: {len(products)} products scraped")
            time.sleep(random.uniform(3, 5))  # Random delay between categories
            
        print(f"\nTotal products scraped: {len(all_products)}")
        print("Saving data...")
        self.save_data(all_products)
        self.driver.quit()

if __name__ == "__main__":
    scraper = SheinScraper()
    try:
        scraper.run()
    except Exception as e:
        print(f"Scraping failed: {e}")
    finally:
        scraper.driver.quit()