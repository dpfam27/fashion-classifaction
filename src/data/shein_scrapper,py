import requests
from bs4 import BeautifulSoup
import pandas as pd
import os
import time
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import random

class SheinScraper:
    def __init__(self):
        # Define paths
        self.base_dir = os.path.join('data', 'raw', 'shein_data')
        self.train_dir = os.path.join(self.base_dir, 'train')
        self.test_dir = os.path.join(self.base_dir, 'test')
        
        # Create directories if they don't exist
        for dir in [
            self.train_dir, 
            self.test_dir,
            os.path.join(self.train_dir, 'images'),
            os.path.join(self.test_dir, 'images')
        ]:
            os.makedirs(dir, exist_ok=True)
        
        # Setup webdriver
        self.driver = webdriver.Chrome()
        self.base_url = "https://s1.shein.com/vn"
        
        # Categories to scrape
        self.categories = [
            'dresses',
            'tops',
            'bottoms'
        ]
        
    def scrape_category(self, category, count=500):
        url = f"{self.base_url}/category/{category}"
        products = []
        
        try:
            self.driver.get(url)
            
            while len(products) < count:
                # Wait for products to load
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_all_elements_located(
                        (By.CLASS_NAME, 'S-product-item')
                    )
                )
                
                # Find all products
                items = self.driver.find_elements(
                    By.CLASS_NAME, 'S-product-item'
                )
                
                for item in items:
                    if len(products) >= count:
                        break
                        
                    try:
                        product = {
                            'title': item.find_element(
                                By.CLASS_NAME, 'S-product-item__name'
                            ).text,
                            'price': item.find_element(
                                By.CLASS_NAME, 'S-product-item__price'
                            ).text,
                            'image_url': item.find_element(
                                By.TAG_NAME, 'img'
                            ).get_attribute('src'),
                            'product_url': item.find_element(
                                By.TAG_NAME, 'a'
                            ).get_attribute('href'),
                            'category': category
                        }
                        products.append(product)
                        
                    except Exception as e:
                        print(f"Error extracting product: {e}")
                        continue
                
                # Scroll for more products
                self.driver.execute_script(
                    "window.scrollTo(0, document.body.scrollHeight);"
                )
                time.sleep(2)
                
        except Exception as e:
            print(f"Error scraping category {category}: {e}")
            
        return products
    
    def save_data(self, products):
        """Split data into train/test and save"""
        # Shuffle products
        random.shuffle(products)
        split_idx = len(products) // 2
        
        # Split into train/test
        train_products = products[:split_idx]
        test_products = products[split_idx:]
        
        # Save train data
        self._save_subset(train_products, 'train')
        # Save test data
        self._save_subset(test_products, 'test')
        
    def _save_subset(self, products, subset='train'):
        """Save a subset (train/test) of products"""
        dir_path = self.train_dir if subset == 'train' else self.test_dir
        
        # Save images
        for idx, product in enumerate(products):
            image_path = os.path.join(dir_path, 'images', f'{subset}_{idx}.jpg')
            self._download_image(product['image_url'], image_path)
            product['local_path'] = image_path
            
        # Save metadata
        df = pd.DataFrame(products)
        df.to_csv(os.path.join(dir_path, f'{subset}.csv'), index=False)
        
    def _download_image(self, url, path):
        """Download image from URL"""
        try:
            response = requests.get(url)
            if response.status_code == 200:
                with open(path, 'wb') as f:
                    f.write(response.content)
                return True
        except Exception as e:
            print(f"Error downloading image {url}: {e}")
        return False
    
    def run(self):
        """Run complete scraping process"""
        all_products = []
        
        for category in self.categories:
            print(f"Scraping category: {category}")
            products = self.scrape_category(category)
            all_products.extend(products)
            time.sleep(2)  # Delay between categories
            
        print(f"Total products scraped: {len(all_products)}")
        self.save_data(all_products)
        self.driver.quit()

if __name__ == "__main__":
    scraper = SheinScraper()
    scraper.run()